{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/tensor_train.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m tensor_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./output/tensor_train.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Táº£i tensor_labels\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tensor_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/tensor_labels.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/tensor_train.pt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(data):\n",
    "    data[:,:,468:543] = 1 - data[:,:,468:543]\n",
    "    return data\n",
    "class Preprocessing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Preprocessing, self).__init__()\n",
    "\n",
    "    def normalize(self,x):\n",
    "        nonan = x[~torch.isnan(x)].view(-1, x.shape[-1])\n",
    "        x = x - nonan.mean(0)[None, None, :]\n",
    "        x = x / nonan.std(0, unbiased=False)[None, None, :]\n",
    "        return x\n",
    "    \n",
    "    def fill_nans(self,x):\n",
    "        x[torch.isnan(x)] = 0\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #seq_len, 3* n_landmarks -> seq_len, n_landmarks, 3\n",
    "        x = x.reshape(x.shape[0],3,-1).permute(0,2,1)\n",
    "        \n",
    "        # Normalize & fill nans\n",
    "        x = self.normalize(x)\n",
    "        x = self.fill_nans(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6141, 0.5965, 0.5886, 0.5864, 0.5830, 0.5987, 0.5946, 0.5918,\n",
       "          0.5900, 0.6100, 0.6048, 0.6008, 0.5977, 0.6193, 0.6150, 0.6112,\n",
       "          0.6080, 0.6266, 0.6252, 0.6224, 0.6197, 0.5029, 0.5140, 0.5204,\n",
       "          0.5247, 0.4938, 0.4870, 0.4803, 0.5348, 0.4723, 0.5172, 0.4910,\n",
       "          0.5928, 0.4162, 0.6233, 0.3800, 0.6149, 0.3877, 0.6189, 0.3831,\n",
       "          0.6031, 0.3969, 0.5987, 0.4016, 0.5560, 0.4474, 0.5535, 0.4515,\n",
       "          0.5462, 0.4421, 0.5463, 0.4388, 0.5324, 0.4509, 0.3859, 0.4033,\n",
       "          0.4106, 0.4124, 0.4137, 0.4030, 0.4057, 0.4074, 0.4085, 0.3915,\n",
       "          0.3948, 0.3979, 0.3998, 0.3815, 0.3846, 0.3879, 0.3905, 0.3735,\n",
       "          0.3724, 0.3743, 0.3768]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_data[:1,:1,468:543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data_flip = flip(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3859, 0.4035, 0.4114, 0.4136, 0.4170, 0.4013, 0.4054, 0.4082,\n",
       "          0.4100, 0.3900, 0.3952, 0.3992, 0.4023, 0.3807, 0.3850, 0.3888,\n",
       "          0.3920, 0.3734, 0.3748, 0.3776, 0.3803, 0.4971, 0.4860, 0.4796,\n",
       "          0.4753, 0.5062, 0.5130, 0.5197, 0.4652, 0.5277, 0.4828, 0.5090,\n",
       "          0.4072, 0.5838, 0.3767, 0.6200, 0.3851, 0.6123, 0.3811, 0.6169,\n",
       "          0.3969, 0.6031, 0.4013, 0.5984, 0.4440, 0.5526, 0.4465, 0.5485,\n",
       "          0.4538, 0.5579, 0.4537, 0.5612, 0.4676, 0.5491, 0.6141, 0.5967,\n",
       "          0.5894, 0.5876, 0.5863, 0.5970, 0.5943, 0.5926, 0.5915, 0.6085,\n",
       "          0.6052, 0.6021, 0.6002, 0.6185, 0.6154, 0.6121, 0.6095, 0.6265,\n",
       "          0.6276, 0.6257, 0.6232]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_data_flip[:1,:1,468:543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
