{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'dropout' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.model import Net\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = Net(num_block=3, num_class=50, num_landmark=390, max_length=124, embed_dim=512, num_head=16,in_channels=390,kernel_size=17)\n",
    "checkpoint = torch.load('output/weights/cfg_3/_42/checkpoint_best_seed-1.pth', map_location=torch.device('cpu'))\n",
    "model_state_dict = checkpoint['model']\n",
    "model.load_state_dict(model_state_dict)\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "input_data = torch.randn(1,124, 390)\n",
    "model(input_data)\n",
    "# Chuyển model sang ONNX\n",
    "torch.onnx.export(model,                      # model\n",
    "                    input_data,                # dữ liệu đầu vào mẫu\n",
    "                    \"output/model.onnx\",              # tên file output\n",
    "                    export_params=True,        # chuyển cả trọng số của model\n",
    "                    do_constant_folding=True,  # folding các biến hằng trong model để tối ưu\n",
    "                    input_names=['input'],     # tên của các đầu vào của model\n",
    "                    output_names=['output'])   # tên của các đầu ra của model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 124, 390)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def interpolate_or_pad(data, max_len=100, mode=\"start\"):\n",
    "    diff = max_len - data.shape[0]\n",
    "    if diff <= 0:  # Crop\n",
    "        data = F.interpolate(data.permute(1,2,0),max_len).permute(2,0,1)\n",
    "        mask = torch.ones_like(data[:,0,0])\n",
    "        return data, mask\n",
    "    coef = 0\n",
    "    padding = torch.ones((diff, data.shape[1], data.shape[2]))\n",
    "    mask = torch.ones_like(data[:,0,0])\n",
    "    data = torch.cat([data, padding * coef])\n",
    "    mask = torch.cat([mask, padding[:,0,0] * coef])\n",
    "    return data, mask\n",
    "class Preprocessing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Preprocessing, self).__init__()\n",
    "    def normalize(self,x):\n",
    "        nonan = x[~torch.isnan(x)].view(-1, x.shape[-1])\n",
    "        x = x - nonan.mean(0)[None, None, :]\n",
    "        x = x / nonan.std(0, unbiased=False)[None, None, :]\n",
    "        return x\n",
    "    def fill_nans(self,x):\n",
    "        x[torch.isnan(x)] = 0\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #seq_len, 3* n_landmarks -> seq_len, n_landmarks, 3\n",
    "        x = x.reshape(x.shape[0],3,-1).permute(0,2,1)\n",
    "        # Normalize & fill nans\n",
    "        x = self.normalize(x)\n",
    "        x = self.fill_nans(x)\n",
    "        return x\n",
    "preprocess = Preprocessing()\n",
    "x = np.load('./dataset/train_landmarks_npy/VSL50/bucminh_pg_1.npy')\n",
    "x = torch.from_numpy(x)\n",
    "x = preprocess(x)\n",
    "x, mask = interpolate_or_pad(x, max_len=124)\n",
    "x = x.reshape(x.shape[0],-1)\n",
    "x = x.unsqueeze(0)\n",
    "x = x.numpy().astype(np.float32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: không cho\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import onnxruntime\n",
    "sess = onnxruntime.InferenceSession(\"output/model.onnx\")\n",
    "def predict(input_data, session, label_map,threshold=0.4):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    # Chạy mô hình trên dữ liệu đầu vào\n",
    "    output = session.run(None, {'input': input_data})[0][0]\n",
    "    predictions = np.argmax(output, axis=0)\n",
    "    probabilities = (np.exp(output) / np.sum(np.exp(output), axis=0))\n",
    "    confidence = probabilities[np.argmax(probabilities, axis=0)]\n",
    "    predicted_labels = list(label_map.keys())[predictions]\n",
    "    if confidence < threshold :\n",
    "        return \"Uncertain\"\n",
    "    else:\n",
    "        return predicted_labels\n",
    "with open('dataset/sign_to_prediction_index_map.json', 'r', encoding='utf-8') as json_file:\n",
    "    label_map = json.load(json_file)\n",
    "\n",
    "ranx = np.random.randn(124,390)\n",
    "predicted_labels = predict(ranx, sess, label_map,0.7)\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks Audio API.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[0;32m     21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifier\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\audio_classifier.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_options_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_audio_task_api\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_data \u001b[38;5;28;01mas\u001b[39;00m audio_data_module\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_result \u001b[38;5;28;01mas\u001b[39;00m classification_result_module\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\core\\base_audio_task_api.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_record\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptional_dependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     27\u001b[0m _TaskRunner \u001b[38;5;241m=\u001b[39m task_runner_module\u001b[38;5;241m.\u001b[39mTaskRunner\n\u001b[0;32m     28\u001b[0m _Packet \u001b[38;5;241m=\u001b[39m packet_module\u001b[38;5;241m.\u001b[39mPacket\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\core\\optional_dependencies.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TensorFlow isn't a dependency of mediapipe pip package. It's only\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# required in the API docgen pipeline so we'll ignore it if tensorflow is not\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# installed.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;66;03m# Replace the real doc_controls.do_not_generate_docs with an no-op\u001b[39;00m\n\u001b[0;32m     23\u001b[0m   doc_controls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:40\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import onnxruntime\n",
    "\n",
    "sess = onnxruntime.InferenceSession(\"output/model.onnx\")\n",
    "def predict(input_data, session, label_map,threshold=0.5):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    # Chạy mô hình trên dữ liệu đầu vào\n",
    "    output = session.run(None, {'input': input_data})[0][0]\n",
    "    predictions = np.argmax(output, axis=0)\n",
    "    probabilities = (np.exp(output) / np.sum(np.exp(output), axis=0))\n",
    "    confidence = probabilities[np.argmax(probabilities, axis=0)]\n",
    "    predicted_labels = list(label_map.keys())[predictions]\n",
    "    if confidence < threshold :\n",
    "        return \"Uncertain\"\n",
    "    else:\n",
    "        return predicted_labels\n",
    "with open('dataset/sign_to_prediction_index_map.json', 'r', encoding='utf-8') as json_file:\n",
    "    label_map = json.load(json_file)\n",
    "\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [ 0, \n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "\n",
    "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
    "LPOSE = [513,505,503,501]\n",
    "RPOSE = [512,504,502,500]\n",
    "\n",
    "LARMS = [501, 503, 505, 507, 509, 511]\n",
    "RARMS = [500, 502, 504, 506, 508, 510]\n",
    "\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE + LARMS + RARMS\n",
    "def extract_keypoints(results,POINT_LANDMARKS):\n",
    "    face_x  = np.array([res.x for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    face_y  = np.array([res.y for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    face_z  = np.array([res.z for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    \n",
    "    lh_x  = np.array([res.x for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    lh_y  = np.array([res.y for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    lh_z  = np.array([res.z for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    \n",
    "    pose_x  = np.array([res.x for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    pose_y  = np.array([res.y for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    pose_z  = np.array([res.z for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    \n",
    "    \n",
    "    rh_x  = np.array([res.x for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "    rh_y  = np.array([res.y for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "    rh_z  = np.array([res.z for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "\n",
    "    x_cor = np.concatenate([face_x, lh_x, pose_x, rh_x])\n",
    "    y_cor = np.concatenate([face_y, lh_y, pose_y, rh_y])\n",
    "    z_cor = np.concatenate([face_z, lh_z, pose_z, rh_z])\n",
    "    POINT_LANDMARKS_array = np.array(POINT_LANDMARKS)\n",
    "    result = np.concatenate((x_cor[POINT_LANDMARKS_array], y_cor[POINT_LANDMARKS_array], z_cor[POINT_LANDMARKS_array]))\n",
    "    return   result\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "n_frames = 0\n",
    "# Mở camera\n",
    "cap = cv2.VideoCapture('./boiroi_tx_2.mp4')\n",
    "inputs_model = np.zeros((124, 390))\n",
    "while cap.isOpened():\n",
    "    # Đọc frame từ camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Không thể đọc từ camera.\")\n",
    "        break\n",
    "    \n",
    "    # Chuyển đổi frame sang màu xám để xử lý nhanh hơn\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Dùng Mediapipe để xử lý landmarks trên khuôn mặt và bàn tay\n",
    "    results = holistic.process(image=frame)\n",
    "    keypoint = extract_keypoints(results)\n",
    "    inputs_model = np.vstack((keypoint, inputs_model))\n",
    "    if inputs_model.shape[0] >= 124:\n",
    "        inputs_model =  inputs_model[:124]\n",
    "    predicted_labels = predict(inputs_model, sess, label_map, 0.7)\n",
    "    print(predicted_labels)\n",
    "    # Hiển thị frame\n",
    "    cv2.imshow('MediaPipe Holistic', frame)\n",
    "    n_frames +=1\n",
    "    print(n_frames)\n",
    "    # Thoát khỏi vòng lặp nếu nhấn phím 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Giải phóng các tài nguyên\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Preprocessing, self).__init__()\n",
    "\n",
    "    def normalize(self,x):\n",
    "        nonan = x[~torch.isnan(x)].view(-1, x.shape[-1])\n",
    "        x = x - nonan.mean(0)[None, None, :]\n",
    "        x = x / nonan.std(0, unbiased=False)[None, None, :]\n",
    "        return x\n",
    "    \n",
    "    def fill_nans(self,x):\n",
    "        x[torch.isnan(x)] = 0\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        #seq_len, 3* n_landmarks -> seq_len, n_landmarks, 3\n",
    "        x = x.reshape(x.shape[0],3,-1).permute(0,2,1)\n",
    "        \n",
    "        # Normalize & fill nans\n",
    "        x = self.normalize(x)\n",
    "        x = self.fill_nans(x)\n",
    "        return x\n",
    "def interpolate_or_pad(data, max_len=124, mode=\"start\"):\n",
    "    diff = max_len - data.shape[0]\n",
    "\n",
    "    if diff <= 0:  # Crop\n",
    "        data = F.interpolate(data.permute(1,2,0),max_len).permute(2,0,1)\n",
    "        return data\n",
    "    coef = 0\n",
    "    padding = torch.ones((diff, data.shape[1], data.shape[2]))\n",
    "    data = torch.cat([data, padding * coef])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 390)\n",
      "torch.Size([124, 390])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bối rối'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = inputs_model[:87]\n",
    "print(x.shape)\n",
    "x = interpolate_or_pad(preprocess(x))\n",
    "x = x.reshape(x.shape[0],-1)\n",
    "print(x.shape)\n",
    "predicted_labels = predict(x, sess, label_map, 0.3)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 390)\n",
      "torch.Size([124, 390])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bối rồi'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load('./boiroi_tx_2.npy')\n",
    "print(x.shape)\n",
    "x = preprocess(x)\n",
    "x, _ = interpolate_or_pad(x, max_len=124)\n",
    "x = x.reshape(x.shape[0],-1)\n",
    "print(x.shape)\n",
    "predicted_labels = predict(x, sess, label_map, 0.3)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 390)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_model[:60].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# POINT_LANDMARKS_idx =np.concatenate((np.array(POINT_LANDMARKS), np.array(POINT_LANDMARKS)+543, np.array(POINT_LANDMARKS)+543*2))\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m extract_keypoints(\u001b[43mresults\u001b[49m,POINT_LANDMARKS)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [ 0, \n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
    "LPOSE = [513,505,503,501]\n",
    "RPOSE = [512,504,502,500]\n",
    "LARMS = [501, 503, 505, 507, 509, 511]\n",
    "RARMS = [500, 502, 504, 506, 508, 510]\n",
    "\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE + LARMS + RARMS \n",
    "def extract_keypoints(results, POINT_LANDMARKS):\n",
    "    face_x  = np.array([res.x for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    face_y  = np.array([res.y for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    face_z  = np.array([res.z for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    \n",
    "    lh_x  = np.array([res.x for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    lh_y  = np.array([res.y for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    lh_z  = np.array([res.z for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    \n",
    "    pose_x  = np.array([res.x for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    pose_y  = np.array([res.y for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    pose_z  = np.array([res.z for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    \n",
    "    \n",
    "    rh_x  = np.array([res.x for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "    rh_y  = np.array([res.y for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "    rh_z  = np.array([res.z for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "\n",
    "    x_cor = np.concatenate([face_x, lh_x, pose_x, rh_x])\n",
    "    y_cor = np.concatenate([face_y, lh_y, pose_y, rh_y])\n",
    "    z_cor = np.concatenate([face_z, lh_z, pose_z, rh_z])\n",
    "    POINT_LANDMARKS_array = np.array(POINT_LANDMARKS)\n",
    "    result = np.concatenate((x_cor[POINT_LANDMARKS_array], y_cor[POINT_LANDMARKS_array], z_cor[POINT_LANDMARKS_array]))\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "# POINT_LANDMARKS_idx =np.concatenate((np.array(POINT_LANDMARKS), np.array(POINT_LANDMARKS)+543, np.array(POINT_LANDMARKS)+543*2))\n",
    "extract_keypoints(results,POINT_LANDMARKS).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m124\u001b[39m, \u001b[38;5;241m390\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     keypoint \u001b[38;5;241m=\u001b[39m extract_keypoints(\u001b[43mresults\u001b[49m)\n\u001b[0;32m      4\u001b[0m     inputs_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((keypoint, inputs_model))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_model\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m124\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "inputs_model = np.zeros((124, 390))\n",
    "for i in range(150):\n",
    "    keypoint = extract_keypoints(results)\n",
    "    inputs_model = np.vstack((keypoint, inputs_model))\n",
    "    if inputs_model.shape[0] >= 124:\n",
    "        inputs_model =  inputs_model[:124]\n",
    "    predicted_labels = predict(inputs_model, sess, label_map, 0.7)\n",
    "    print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 390)\n",
      "(2, 390)\n",
      "(3, 390)\n",
      "(4, 390)\n",
      "(5, 390)\n",
      "(6, 390)\n",
      "(7, 390)\n",
      "(8, 390)\n",
      "(9, 390)\n",
      "(10, 390)\n",
      "(11, 390)\n",
      "(12, 390)\n",
      "(13, 390)\n",
      "(14, 390)\n",
      "(15, 390)\n",
      "(16, 390)\n",
      "(17, 390)\n",
      "(18, 390)\n",
      "(19, 390)\n",
      "(20, 390)\n",
      "(21, 390)\n",
      "(22, 390)\n",
      "(23, 390)\n",
      "(24, 390)\n",
      "(25, 390)\n",
      "(26, 390)\n",
      "(27, 390)\n",
      "(28, 390)\n",
      "(29, 390)\n",
      "(30, 390)\n",
      "(31, 390)\n",
      "(32, 390)\n",
      "(33, 390)\n",
      "(34, 390)\n",
      "(35, 390)\n",
      "(36, 390)\n",
      "(37, 390)\n",
      "(38, 390)\n",
      "(39, 390)\n",
      "(40, 390)\n",
      "(41, 390)\n",
      "(42, 390)\n",
      "(43, 390)\n",
      "(44, 390)\n",
      "(45, 390)\n",
      "(46, 390)\n",
      "(47, 390)\n",
      "(48, 390)\n",
      "(49, 390)\n",
      "(50, 390)\n",
      "(51, 390)\n",
      "(52, 390)\n",
      "(53, 390)\n",
      "(54, 390)\n",
      "(55, 390)\n",
      "(56, 390)\n",
      "(57, 390)\n",
      "(58, 390)\n",
      "(59, 390)\n",
      "(60, 390)\n",
      "(61, 390)\n",
      "(62, 390)\n",
      "(63, 390)\n",
      "(64, 390)\n",
      "(65, 390)\n",
      "(66, 390)\n",
      "(67, 390)\n",
      "(68, 390)\n",
      "(69, 390)\n",
      "(70, 390)\n",
      "(71, 390)\n",
      "(72, 390)\n",
      "(73, 390)\n",
      "(74, 390)\n",
      "(75, 390)\n",
      "(76, 390)\n",
      "(77, 390)\n",
      "(78, 390)\n",
      "(79, 390)\n",
      "(80, 390)\n",
      "(81, 390)\n",
      "(82, 390)\n",
      "(83, 390)\n",
      "(84, 390)\n",
      "(85, 390)\n",
      "(86, 390)\n",
      "(87, 390)\n",
      "Không thể đọc từ camera.\n",
      "bối rối\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import onnxruntime\n",
    "from data.post_process import Preprocessing, interpolate_or_pad\n",
    "import sys\n",
    "sys.path.insert(0, 'F:\\6.Spring_24\\VIetNamese_sign_language')\n",
    "from dataset.extract_landmark import POINT_LANDMARKS\n",
    "import pandas as pd\n",
    "import json\n",
    "preprocessLayer = Preprocessing()\n",
    "sess = onnxruntime.InferenceSession(\"output/model.onnx\")\n",
    "with open('dataset/sign_to_prediction_index_map.json', 'r', encoding='utf-8') as json_file:\n",
    "    label_map = json.load(json_file)\n",
    "def predict(input_data, session, label_map=label_map,threshold=0.5):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    # Chạy mô hình trên dữ liệu đầu vào\n",
    "    output = session.run(None, {'input': input_data})[0][0]\n",
    "    predictions = np.argmax(output, axis=0)\n",
    "    probabilities = (np.exp(output) / np.sum(np.exp(output), axis=0))\n",
    "    confidence = probabilities[np.argmax(probabilities, axis=0)]\n",
    "    predicted_labels = list(label_map.keys())[predictions]\n",
    "    if confidence < threshold :\n",
    "        return \"Uncertain\"\n",
    "    else:\n",
    "        return predicted_labels\n",
    "def extract_keypoints(results,POINT_LANDMARKS):\n",
    "    face_x  = np.array([res.x for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    face_y  = np.array([res.y for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    face_z  = np.array([res.z for res in results.face_landmarks.landmark],dtype=np.float32) if results.face_landmarks else np.zeros(468)\n",
    "    \n",
    "    lh_x  = np.array([res.x for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    lh_y  = np.array([res.y for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    lh_z  = np.array([res.z for res in results.left_hand_landmarks.landmark],dtype=np.float32) if results.left_hand_landmarks else np.zeros(21)\n",
    "    \n",
    "    pose_x  = np.array([res.x for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    pose_y  = np.array([res.y for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    pose_z  = np.array([res.z for res in results.pose_landmarks.landmark],dtype=np.float32) if results.pose_landmarks else np.zeros(33)\n",
    "    \n",
    "    \n",
    "    rh_x  = np.array([res.x for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "    rh_y  = np.array([res.y for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "    rh_z  = np.array([res.z for res in results.right_hand_landmarks.landmark],dtype=np.float32) if results.right_hand_landmarks else np.zeros(21)\n",
    "\n",
    "    x_cor = np.concatenate([face_x, lh_x, pose_x, rh_x])\n",
    "    y_cor = np.concatenate([face_y, lh_y, pose_y, rh_y])\n",
    "    z_cor = np.concatenate([face_z, lh_z, pose_z, rh_z])\n",
    "    POINT_LANDMARKS_array = np.array(POINT_LANDMARKS)\n",
    "    result = np.concatenate((x_cor[POINT_LANDMARKS_array], y_cor[POINT_LANDMARKS_array], z_cor[POINT_LANDMARKS_array]))\n",
    "    return   result\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "preprocess = Preprocessing()\n",
    "# Mở camera\n",
    "cap = cv2.VideoCapture('./boiroi_tx_2.mp4')\n",
    "sequence=[]\n",
    "n_frame = 0\n",
    "num_frame_space = 15\n",
    "predicted = ''\n",
    "while cap.isOpened():\n",
    "    # Đọc frame từ camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Không thể đọc từ camera.\")\n",
    "        break\n",
    "    \n",
    "    # Chuyển đổi frame sang màu xám để xử lý nhanh hơn\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Dùng Mediapipe để xử lý landmarks trên khuôn mặt và bàn tay\n",
    "    results = holistic.process(image=frame)\n",
    "    keypoints = extract_keypoints(results,POINT_LANDMARKS)\n",
    "    sequence.append(keypoints)\n",
    "    sequence_arr = np.array(sequence)\n",
    "    print(sequence_arr.shape)\n",
    "sequence_arr = interpolate_or_pad(preprocess(sequence_arr))\n",
    "sequence_arr = sequence_arr.reshape(sequence_arr.shape[0],-1)\n",
    "predicted = predict(sequence_arr,sess)\n",
    "print(predicted)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
